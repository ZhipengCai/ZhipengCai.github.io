<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Zhipeng Cai</title>
	<meta content="Zhipeng Cai, ZhipengCai.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
a {
  color: #1772d0;
  text-decoration:none;
}
a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}
a.paper {
  font-weight: bold;
  font-size: 12pt;
}
b.paper {
  font-weight: bold;
  font-size: 12pt;
}
* {
  margin: 0pt;
  padding: 0pt;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1600px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}
h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}
h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}
strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}
		
Highlight {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight:bold;
	color: red;
}
		
ul { 
  list-style: circle;
}
img {
  border: none;
}
li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}
em, i {
	font-style:italic;
}
div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}
div.spanner {
  clear: both;
}
div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
div.paper div {
  padding-left: 230px;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}
span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}
pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}
div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');
</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 180px;">
<div style="margin: 0px auto; width: 100%;">
<img title="ZhipengCai"style="float: left; padding-left: .01em; height: 180px;" src="avatar2.jpg" />
<div style="padding-left: 20em; vertical-align: top; height: 120px;">
<span style="line-height: 150%; font-size: 20pt;">蔡志鹏（Zhipeng Cai）</span><br />
<span>Senior Researcher at Meta, Menlo Park, California, USA. </span><br />
<span>Education: PhD of Computer Science at <a href='https://www.adelaide.edu.au/'>The University of Adelaide</a>, Australia. Supervisor: Prof. <a href='http://cs.adelaide.edu.au/~tjchin/doku.php'>Tat-Jun Chin</a> &amp; Prof. <a href='https://cs.adelaide.edu.au/~dsuter/'>David Suter</a>. </span> <br /> 
<span><Highlight>Email</Highlight>: <strong> czptc2h@gmail.com </strong> </span> <br /> 
<span><a href='https://scholar.google.com/citations?user=AZNUIDAAAAAJ&hl=en'>Google Scholar</a></span><br />
<span>I have supervised Ph.D students/interns from various countries. Send me an email if you are interested in internships/collaborations.</span><br />
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<!--
<div style="clear: both;">
<div class="section">
<h2>About Me (<a href='cv.pdf'>CV</a>)</h2>
<div class="paper"> 
	coming soon...
<br> <br>
</div>
</div>
</div>
-->
	
	<!--
<div style="clear: both;">
<div class="section">
  <h2>Note</h2>
  <div class="paper">
    <ul>
	<li>Some papers are linked to the arxiv version due to some modifications after publication. Please follow the link to prevent misunderstandings :).</li>
    </ul>
  </div>
</div>
</div>
	-->
<div style="clear: both;">
<div class="section">
  <h2>About me</h2>
  <div class="paper">
    <ul>
	<span> 
		I am a senior researcher at Meta. I am interested in general CV/ML problems such as optimization, perception, and multi-modal generation. Several of my representative works have been selected as top-tier conference oral/spotlights. One of them has been selected as one of the 12 best papers at ECCV18. Beyond publication, I am also motivated to develop methods that have practical industrial impacts. I have led several projects that have 1) received considerable amount of github stars, 2) been featured by Intel company news or 3) selected as representative product live demos. I have won the 2024 best scholar award at Intel Labs.
	</span> 	
    </ul>
  </div>
</div>
</div>
	
<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
	<li> <img src="hot_wide.png" class="media" alt="" height = "17" width="39" /> Mar-2025: MonSter is selected as <Highlight> CVPR 2025 highlight (<3% acceptance rate)! </Highlight> </li>
	<li> <img src="hot_wide.png" class="media" alt="" height = "17" width="39" /> Feb-2025: <a href='https://arxiv.org/pdf/2501.08643'>Paper</a> accepted to <a href = 'https://cvpr.thecvf.com/'> CVPR2025 </a>. Topic: Stereo Matching. Our method MonSter ranked the 1st across 5 widely used leaderboards, significantly advanced current SOTA! </li>
	<li> <img src="hot_wide.png" class="media" alt="" height = "17" width="39" /> Dec-2024: Paper accepted to <a href = 'https://aaai.org/conference/aaai/aaai-25/'> AAAI2025 </a>. Topic: Lifelong visual localization. </li>    
	<li> <img src="hot_wide.png" class="media" alt="" height = "17" width="39" /> Nov-2024: I am starting a new position at Meta as a senior AI research scientist! </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Sep-2024: Two papers accepted to <a href = 'https://nips.cc/'> NeurIPS2024 </a>. Topic: 1) Articulated 3D generation; 2) Neuromorphic-based Optimization </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Aug-2024: <a href = 'https://arxiv.org/pdf/2404.15506'>Metric3D v2</a> accepted to TPAMI! </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> July-2024: LiSA featured by <a href = 'https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Research-on-LiDAR-Localization-with-Semantic-Awareness/post/1619106'> Intel Blog Post </a> and <a href='https://www.linkedin.com/feed/update/urn:li:activity:7226255742540865536/'>Intel Labs Linedin</a>! </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> June-2024: L-MAGIC <a href = 'https://huggingface.co/spaces/MMPano/MMPano'> Huggingface Demo </a> now available! </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> June-2024: L-MAGIC featured by <a href = 'https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Advancing-Gen-AI-on-Intel-Gaudi-AI-Accelerators-with-Multi-Modal/post/1603746'> Intel Blog Post </a> and <a href='https://www.linkedin.com/feed/update/urn:li:activity:7203797143831076864/'>Intel Labs Linedin</a>! </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> May-2024: L-MAGIC fully enabled at Intel Gaudi 2 and selected as <strong>one of the 5 Intel featured <a href = https://www.youtube.com/watch?v=ZC2hwi3O0eo&t=0s> live demos </a> </strong> at <a href = https://www.intel.com/content/www/us/en/events/supercomputing.html> ISC HPC 2024 </a>! </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> May-2024: <a href = https://arxiv.org/abs/2304.04795> Paper </a> accepted to <a href = https://icml.cc/> ICML2024! </a> Topic: Test Time Adaptation. </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Apr-2024: LiSA is selected as <Highlight> CVPR 2024 highlight (3.6% acceptance rate)! </Highlight> </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Feb-2024: GIM is featured at the <a href = 'https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Research-Work-Receives-Spotlight-Award-at-Top-AI/post/1575985?emcs_t=S2h8ZW1haWx8dG9waWNfc3Vic2NyaXB0aW9ufExVMUlXU1EwMVRUVUpQfDE1NzU5ODV8U1VCU0NSSVBUSU9OU3xoSw'> Intel Blog Post </a>. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Jan-2024: Two papers accepted to <a href = 'https://cvpr.thecvf.com/'> CVPR2024 </a>. Topic: 1) Anything-to-panorama using pre-trained LLMs to control 2D Diffusion models. 2) LiDAR localization with semantic knowledge distillation. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Jan-2024: Paper accepted to <a href = 'https://iclr.cc/'> ICLR2024 </a> as <Highlight> spotlight </Highlight> (top 5%). Topic: Zero-shot Image Matching. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Dec-2023: Paper accepted to <a href = 'https://aaai.org/aaai-conference/'> AAAI2024 </a>. Topic: Continual learning using simulated data. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Oct-2023: Paper accepted to <a href = 'https://neurips.cc/virtual/2023/workshop/66539'> NeurIPS2023 workshop on diffusion models </a>. Topic: Diffusion model for immersive scene generation. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Sep-2023: Two Papers accepted to <a href = 'https://nips.cc/'> NeurIPS2023 </a>. Topic: Event camera to point cloud registration, correspondence NeRF. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Aug-2023: Paper accepted to <a href = 'https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36'> IEEE Transactions on Geoscience and Remote Sensing (TGRS) </a>. Topic: object detection in remote sensing images. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> July-2023: Two papers accepted to <a href = 'https://iccv2023.thecvf.com/'> ICCV 2023 </a>. Topic: 1. continual learning on NeRFs 2. zero-shot metric depth estimation. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> June-2023: Paper accepted to <a href = 'https://sites.google.com/view/clvision2023'> CVPR 2023 Workshop on Continual Learning in Computer Vision </a>. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> May-2022: Co-organizing the <a href = 'https://sites.google.com/andrew.cmu.edu/gpr-competition/'> ICRA 2022 Gereral Place Recognition Competition </a>. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> July-2021: Paper accepted to <a href='http://iccv2021.thecvf.com/home'> ICCV2021 </a>! Topic: online continual learning. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Sep-2020: Got my Ph.D degree (my <a href='https://digital.library.adelaide.edu.au/dspace/bitstream/2440/127452/1/Cai2020_PhD.pdf'>thesis</a> was awarded the <a href='https://www.adelaide.edu.au/graduatecentre/current-students/your-thesis-examination/research-student-excellence-awards'>Dean’s Commendation for Doctoral Thesis Excellence</a>)! Now a postdoc at Intel Intelligent Systems Lab. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> June-2020: Served as a "wingman" during TJ's <a href='http://cmp.felk.cvut.cz/cvpr2020-ransac-tutorial/'> RANSAC CVPR 2020 Tutorial </a>. Find me <a href='https://youtu.be/WkN3FP_jbuI/'> here </a> </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> July-2019: Paper accepted to IJCV Special Issue on Best of ECCV 2018 </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> July-2019: Paper accepted as <Highlight> oral (4.6% acceptance rate) </Highlight> to <a href='https://http://iccv2019.thecvf.com/'> ICCV2019 </a>! Subject: robust fitting. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Jan-2019 to July-2019: Internship at <a href = 'http://vladlen.info/lab/'>Intel Intelligent Systems Lab</a> at Silicon Valley. Working with Dr. <a href = 'http://vladlen.info/'>Vladlen Koltun</a>. </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Nov-2018: Paper "Robust fitting in computer vision: easy or hard?" has been selected as <Highlight>one of the 12 best papers of ECCV2018 (0.4% acceptance rate) </Highlight> </li>
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> Nov-2018: Paper accepted to <a href='https://www.journals.elsevier.com/isprs-journal-of-photogrammetry-and-remote-sensing'>ISPRS Journal of Photogrammetry and Remote Sensing </a>. Subject: point cloud registration. </li>    
	<li> <img src="blank_wide.png" class="media" alt="" height = "17" width="39" /> July-2018: Two papers accepted as <Highlight> oral (2.4% acceptance rate) </Highlight> to <a href='https://eccv2018.org/'> ECCV2018 </a>! Subject: robust fitting. </li>
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Award</h2>
<div class="paper">
<ul>

<li>2025 <a href='https://cvpr.thecvf.com/'>CVPR25</a> <Highlight>highlight</Highlight> (<3% acceptance rate) </li>
<li>2024 Intel Labs Best Scholar Award (Recognizing the person with the highest overall publication quantity and quality over the year, throughout the whole company.) </li>	
<li>2024 <a href='https://cvpr.thecvf.com/'>CVPR24</a> <Highlight>highlight</Highlight> (3.6% acceptance rate) </li>	
<li>2024 <a href='https://iclr.cc/'>ICLR24</a> <a href='https://arxiv.org/abs/2402.11095'><Highlight>spotlight</Highlight> </a> (5% acceptance rate) </li>	
<li>2020 <a href='https://www.adelaide.edu.au/graduatecentre/current-students/your-thesis-examination/research-student-excellence-awards'>Dean’s Commendation for Doctoral Thesis Excellence</a></li>	
<li>2019 <a href='https://http://iccv2019.thecvf.com/'>ICCV19</a> <a href='https://arxiv.org/abs/1908.02021'><Highlight>oral</Highlight></a> (4.6% acceptance rate) </li>	
<li>2018 One of the 12 <Highlight>best <a href='https://arxiv.org/abs/1802.06464'>papers</a></Highlight> in <a href='https://eccv2018.org/'>ECCV'2018</a> (0.4% acceptance rate)</li>	
<li>2018 <a href='https://eccv2018.org/'>ECCV18</a> <a href='https://arxiv.org/abs/1802.06464'><Highlight>oral</Highlight></a> (2.4% acceptance rate) </li>	
<li>2018 <a href='https://eccv2018.org/'>ECCV18</a> <a href='https://arxiv.org/abs/1802.06464'><Highlight>oral</Highlight></a> (2.4% acceptance rate) </li>	
<li>2018 Ranked the first 30th in the <a href='http://www.cvmart.net/activity/detail/1'>Ranking List of 2018 Potential Computer Vision Developers</a></li>	
<li>2016 Excellent Master Degree Graduation Thesis</li>
<li>2013 Excellent Bachelor Degree Graduation Thesis</li>
<li>2013 The CCF Outstanding Undergraduate Award</li>
<li>2012 The Meritorious Winner of 2012 Mathematical Contest In Modeling Certificate of Achievement</li>

</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publication (Check the ArXiv version for modifications after publication)</h2>

<h2 id="confpapers">2025</h2>

<div class="paper" id="CVPR2025"><img class="paper" src="papers/CVPR2025.png" title="CVPR25" />
<div> <strong>[CVPR25 <Highlight>highlight (top 3%)</Highlight>]</strong> MonSter: Marry Monodepth to Stereo Unleashes Power <br />
Junda Cheng, Longliang Liu, Gangwei Xu, Xianqi Wang, Zhaoxing Zhang, Yong Deng, Jinliang Zang, Yurui Chen, <strong>Zhipeng Cai</strong>, Xin Yang <br />
The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025 <br />
<a href='https://arxiv.org/pdf/2501.08643'>[ArXiv]</a>
<a href='https://github.com/Junda24/MonSter'>[Code]</a>
</div>
<iframe width="400" height="225" src="https://www.youtube.com/embed/9VSK_mId5WY?si=m59eZACk-8_cEXU3" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="PAMI2025"><img class="paper" src="papers/PAMI25.png" title="PAMI25" />
<div> RoMeO: Robust Metric Visual Odometry <br />
Junda Cheng<sup>+</sup>, <strong>Zhipeng Cai<sup>+</sup></strong>, Zhaoxing Zhang, Wei Yin, Matthias Muller, Michael Paulitsch, Xin Yang <br />
<sup>+</sup>: Equal contribution <br />
<a href='https://arxiv.org/pdf/2412.11530'>[ArXiv]</a>
<a href='https://github.com/Junda24/RoMeO'>[Code]</a>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="AAAI2025"><img class="paper" src="papers/AAAI25.png" title="AAAI25" />
<div> <strong>[AAAI25]</strong> ConDo: Continual Domain Expansion for Absolute Pose Regression <br />
Zijun Li<sup>+</sup>, <strong> Zhipeng Cai<sup>+, *</sup></strong>, Bochun Yang, Xuelun Shen, Siqi Shen, Xiaoliang Fan, Michael Paulitsch, Cheng Wang<sup>*</sup> <br />
<sup>+</sup>: Equal contribution, <sup>*</sup>: Equal corresponding author. <br />
The 39th Annual AAAI Conference on Artificial Intelligence (AAAI 2025) <br />
<a href='https://arxiv.org/pdf/2412.13452'>[Paper]</a>
<a href='https://github.com/ZijunLi7/ConDo'>[Code]</a>
</div>
<div class="spanner"></div>
</div>

<h2 id="confpapers">2024</h2>
	
<div class="paper" id="NeurIPs24_3d"><img class="paper" src="papers/NeurIPs24_3d.png" title="NeurIPs24_3d" />
<div> <strong>[NeurIPS24]</strong> MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated Designs <br />
Quentin Leboutet, Nina Wiedemann, <strong> Zhipeng Cai </strong>, Michael Paulitsch, Kai Yuan <br />
Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2024 <br />
<a href='https://openreview.net/pdf?id=re2jPCnzkA'>[Paper]</a>
<a href='https://github.com/quentin-leboutet/MIDGArD'>[Code]</a>
<a href='https://quentin-leboutet.github.io/MIDGArD/'>[Project page]</a>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="NeurIPs24_neuro"><img class="paper" src="papers/NeurIPs24_neuro.png" title="NeurIPs24_neuro" />
<div> <strong>[NeurIPS24]</strong>Slack-Free Spiking Neural Network Formulation for Hypergraph Minimum Vertex Cover <br />
Tam Ngoc-Bang Nguyen, Anh-Dzung Doan, <strong> Zhipeng Cai </strong>, Tat-Jun Chin <br />
Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2024 <br />
<a href='https://openreview.net/pdf?id=4A5IQEjG8c'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="TPAMI24"><img class="paper" src="papers/Metric3Dv2.png" title="ArXiv24_Metric3Dv2" />
<div> <strong>[TPAMI]</strong> Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation <br />
Mu Hu, Wei Yin, Chi Zhang, <strong> Zhipeng Cai </strong>, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, Shaojie Shen <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) <br />
<a href='https://arxiv.org/pdf/2404.15506'>[ArXiv preprint]</a>
<a href='https://jugghm.github.io/Metric3Dv2/'>[Project page]</a>
<a href='https://github.com/YvanYin/Metric3D'>[Code]</a>
<a href='https://huggingface.co/spaces/JUGGHM/Metric3D'>[Huggingface Demo]</a>
</div>
<iframe width="400" height="225" src="https://jugghm.github.io/Metric3Dv2/resource_new/media/demo_full.mp4" title="YouTube video player" frameborder="0" allow="clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="ICCV23_TTT"><img class="paper" src="papers/online_TTT.png" title="ArXiv23_TTT" />
<div> <strong>[ICML24]</strong> Evaluation of Test-Time Adaptation Under Computational Time Constraints <br />
Motasem Alfarra, Hani Itani, Alejandro Pardo, Shyma Alhuwaider, Merey Ramazanova, Juan C. Pérez,  <strong> Zhipeng Cai </strong>, Matthias Müller, Bernard Ghanem <br />
Forty-first International Conference on Machine Learning (ICML) 2024 <br />
<a href='https://arxiv.org/pdf/2304.04795.pdf'>[ArXiv preprint]</a>
<a href='https://github.com/MotasemAlfarra/Online_Test_Time_Adaptation'>[Code]</a>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="CVPR24_L_MAGIC"><img class="paper" src="papers/CVPR_L_MAGIC.png" title="CVPR24_LMAGIC" />
<div> <strong>[CVPR24]</strong> L-MAGIC: Language Model Assisted Generation of Images with Coherence <br />
<strong> Zhipeng Cai </strong>, Matthias Müller, Reiner Birkl, Diana Wofk, Shao-Yen Tseng, JunDa Cheng, Gabriela Ben-Melech Stan, Vasudev Lal, Michael Paulitsch <br />	
The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024 <br />
<a href='https://arxiv.org/pdf/2406.01843'>[Paper]</a>
<a href='https://zhipengcai.github.io/MMPano/'>[Project page]</a>
<a href='https://github.com/IntelLabs/MMPano'>[Code]</a>
<a href='https://huggingface.co/spaces/MMPano/MMPano'>[Huggingface Demo]</a>
<a href='https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Advancing-Gen-AI-on-Intel-Gaudi-AI-Accelerators-with-Multi-Modal/post/1603746'>[Intel Featured Blog]</a>
<a href='https://www.linkedin.com/feed/update/urn:li:activity:7203797143831076864/'>[Intel Labs Linedin]</a>	
<br />
<iframe width="400" height="225" src="https://www.youtube.com/embed/XDMNEzH4-Ec?list=PLG9Zyvu7iBa0-a7ccNLO8LjcVRAoMn57s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="400" height="225" src="https://www.youtube.com/embed/ZC2hwi3O0eo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CVPR24_LISA"><img class="paper" src="papers/CVPR_LISA.png" title="CVPR24_LISA" />
<div> <strong>[CVPR24 <Highlight>highlight (top 3.6%)</Highlight>]</strong> LiSA: LiDAR Localization with Semantic Awareness <br />
Bochun Yang, Zijun Li, Wen Li, <strong> Zhipeng Cai <sup>*</sup> </strong>, Chenglu Wen, Yu Zang, Matthias Müller, Cheng Wang <sup>*</sup> <br />
<sup>*</sup>: Equal corresponding author <br />
The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024 <br />
<a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_LiSA_LiDAR_Localization_with_Semantic_Awareness_CVPR_2024_paper.pdf'>[Paper]</a>
<a href='https://github.com/Ybchun/LiSA'>[Code]</a>
<a href='https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Research-on-LiDAR-Localization-with-Semantic-Awareness/post/1619106'>[Intel Featured Blog]</a>
<a href='https://www.linkedin.com/feed/update/urn:li:activity:7226255742540865536/'>[Intel Labs Linedin]</a>	

</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="ICLR24"><img class="paper" src="papers/GIM.png" title="ICLR24" />
<div> <strong>[ICLR24 <Highlight>highlight (top 5%)</Highlight>]</strong> GIM: Learning Generalizable Image Matcher From Internet Videos <br />
Xuelun Shen<sup>+</sup>, <strong>Zhipeng Cai<sup>+, *</sup></strong>, Wei Yin<sup>+</sup>, Matthias Müller, Zijun Li, Kaixuan Wang, Xiaozhi Chen, Cheng Wang<sup>*</sup> <br />	
<sup>+</sup>: Equal contribution, <sup>*</sup>: Equal corresponding author. <br />
Twelfth International Conference on Learning Representations (ICLR 2024) <br />
<a href='https://arxiv.org/abs/2402.11095'>[ArXiv preprint]</a>
<a href='https://xuelunshen.com/gim/'>[Project page]</a>
<a href='https://github.com/xuelunshen/gim'>[Code]</a>
<a href='https://huggingface.co/spaces/xuelunshen/gim-online'>[Huggingface Demo]</a>
<a href = 'https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Research-Work-Receives-Spotlight-Award-at-Top-AI/post/1575985?emcs_t=S2h8ZW1haWx8dG9waWNfc3Vic2NyaXB0aW9ufExVMUlXU1EwMVRUVUpQfDE1NzU5ODV8U1VCU0NSSVBUSU9OU3xoSw'>[Intel Blog Post]</a>
<br />
<iframe width="560" height="315" src="https://www.youtube.com/embed/FU_MJLD8LeY?si=2Bv8uisiwMnuCS8B" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>

	
<h2 id="confpapers">2023</h2>

<div class="paper" id="CVPR23"><img class="paper" src="papers/SimCS.png" title="ArXiv23" />
<div> <strong>[AAAI24]</strong> SimCS: Simulation for Online Domain-Incremental Continual Segmentation<br />
Motasem Alfarra, <strong> Zhipeng Cai </strong>, Adel Bibi, Bernard Ghanem, Matthias Muller <br />
Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI 2024) <br />
<a href='https://arxiv.org/pdf/2211.16234.pdf'>[ArXiv preprint]</a>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="NeurIPS23_LDM3D"><img class="paper" src="papers/NeurIPS23_LDM3D.png" title="NeurIPS23_LDM3D" />
<div> <strong>[NeurIPSW 23]</strong> LDM3D-VR: Latent Diffusion Model for 3D VR <br />
Gabriela Ben-Melech Stan, Diana Wofk, Estelle Aflalo, Shao-Yen Tseng, <strong> Zhipeng Cai </strong>, Michael Paulitsch, Vasudev Lal <br />
NeurIPS 2023 Workshop on Diffusion Models <br />
<a href='https://arxiv.org/pdf/2311.03226.pdf'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="NeurIPS23_CorresNeRF"><img class="paper" src="papers/neurips23_CorresNeRF.png" title="NeurIPS23_CorresNeRF" />
<div> <strong>[NeurIPS23]</strong> CorresNeRF: Image Correspondence Priors for Neural Radiance Fields. <br />
Yixing Lao, Xiaogang Xu, <strong>Zhipeng Cai</strong>, Xihui Liu, Hengshuang Zhao. <br />
Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023 <br />
<a href='https://openreview.net/pdf?id=pTCZWSDltG'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="NeurIPS23"><img class="paper" src="papers/NeurIPS23.png" title="NeurIPS23" />
<div> <strong>[NeurIPS23]</strong> E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning <br />
Xiuhong Lin, Changjie Qiu, <strong>Zhipeng Cai</strong>, Siqi Shen, Yu Zang, Weiquan Liu, Xuesheng Bian, Matthias Müller, Cheng Wang <br />
Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023 <br />
<a href='https://openreview.net/pdf?id=yiehppUCO2'>[Paper]</a>
<a href='https://github.com/Xmu-qcj/E2PNet'>[Code]</a>
</div>
<div class="spanner"></div>
</div>

	
	
	
<div class="paper" id="TGRS23"><img class="paper" src="papers/TGRS23.png" title="TGRS23" />
<div> <strong>[TGRS]</strong> GSDDet: Ground Sample Distance Guided Object Detection for Remote Sensing Images <br />
Yunuo Yang, <strong> Zhipeng Cai </strong>, Pinqing Song, Yu Zang, Guanjie Huang, Ming Cheng, Cheng Wang <br />
IEEE Transactions on Geoscience and Remote Sensing (TGRS)<br />
<a href='https://ieeexplore.ieee.org/document/10285472'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>

	
<div class="paper" id="ICCV23_CLNeRF"><img class="paper" src="papers/CLNeRF.png" title="CLNeRF23" />
<div> <strong>[ICCV23]</strong> CLNeRF: Continual Learning Meets NeRF <br />
<strong> Zhipeng Cai </strong>, Matthias Müller <br />
International Conference on Computer Vision (ICCV) 2023<br />
<a href='https://arxiv.org/abs/2308.14816'>[Paper]</a>
<a href='https://github.com/IntelLabs/CLNeRF'>[Code]</a>
<a href='https://huggingface.co/datasets/IntelLabs/WAT-WorldAcrossTime'>[dataset]</a>
<br />
<iframe width="400" height="225" src="https://www.youtube.com/embed/nLRt6OoDGq0?si=Td72VUmRrw6mRDCP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV23_Metric3D"><img class="paper" src="papers/ICCV23_Metric3D.png" title="ICCV23_Metric3D.png" />
<div> <strong>[ICCV23]</strong> Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image <br />
Wei Yin, Chi Zhang, Hao Chen, <strong> Zhipeng Cai </strong>, Xiaozhi Chen, Kaixuan Wang, Gang Yu, Chunhua Shen <br />
International Conference on Computer Vision (ICCV) 2023<br />
<a href='https://arxiv.org/pdf/2307.10984v1.pdf'>[ArXiv preprint]</a>
<a href='https://github.com/YvanYin/Metric3D'>[Code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV23_OCL"><img class="paper" src="papers/OCL23.png" title="ArXiv23_OCL" />
<div> Online Continual Learning Without the Storage Constraint <br />
Ameya Prabhu, <strong> Zhipeng Cai </strong>, Puneet Dokania, Philip Torr, Vladlen Koltun, Ozan Sener <br />
<a href='https://arxiv.org/pdf/2305.09253.pdf'>[ArXiv preprint]</a>
<a href='https://github.com/drimpossible/ACM'>[Code]</a>
</div>
<div class="spanner"></div>
</div>
	
<h2 id="confpapers">2022</h2>
<div class="paper" id="CVPR23"><img class="paper" src="papers/SimCS.png" title="ArXiv23" />
<div> <strong>[CVPRW23]</strong> SimCS: Simulation for Online Domain-Incremental Continual Segmentation<br />
Motasem Alfarra, <strong> Zhipeng Cai </strong>, Adel Bibi, Bernard Ghanem, Matthias Muller <br />
CVPR Workshop on Continual Learning (CLVision) 2023 <br />
<a href='https://arxiv.org/pdf/2211.16234.pdf'>[ArXiv preprint]</a>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="ICLR23"><img class="paper" src="papers/AMA.png" title="ArXiv22" />
<div> Improving Information Retention in Large Scale Online Continual Learning<br />
<strong>Zhipeng Cai</strong>, Vladlen Koltun, Ozan Sener <br />
<a href='https://arxiv.org/pdf/2210.06401.pdf'>[ArXiv preprint]</a>
</div>
<div class="spanner"></div>
</div>
	
<h2 id="confpapers">2021</h2>
<div class="paper" id="ICCV21"><img class="paper" src="papers/CLOC.png" title="ICCV21" />
<div> <strong>[ICCV21]</strong> Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data<br />
<strong>Zhipeng Cai</strong>, Ozan Sener, Vladlen Koltun <br />
International Conference on Computer Vision (ICCV) 2021<br />
<a href='https://arxiv.org/pdf/2108.09020.pdf'>[ArXiv preprint]</a>
<a href='https://github.com/IntelLabs/continuallearning/tree/main/CLOC'>[Code]</a>
<br />
<iframe width="400" height="225" src="https://www.youtube.com/embed/-FO2Khwjprg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>
	
<h2 id="confpapers">2020</h2>

<div class="paper" id="Thesis">
<div> Consensus Maximization: Theoretical Analysis and New Algorithms<br />
<strong> Zhipeng Cai </strong> <br />
Ph.D thesis (Supervised by: Prof. Tat-Jun Chin and Prof. David Suter) <br />
<a href='https://digital.library.adelaide.edu.au/dspace/bitstream/2440/127452/1/Cai2020_PhD.pdf'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>

	
<div class="paper" id="ECCV20">
<div> <strong>[ECCV20]</strong> Globally Optimal and Efficient Vanishing Point Estimation in Atlanta World<br />
Haoang Li, Pyojin Kim, Ji Zhao, Kyungdon Joo, <strong>Zhipeng Cai</strong>, Zhe Liu, Yun-Hui Liu <br />
European Conference on Computer Vision (ECCV) 2020<br />
<a href='https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670154.pdf'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>

	
<h2 id="confpapers">2019</h2>

<div class="paper" id="JICV19_Hardness"><img class="paper" src="papers/ECCV18_Hardness.png" title="IJCV19_Hardness" />
<div> <strong>[IJCV]</strong> Robust fitting in computer vision: easy or hard?<br />
Tat-Jun Chin, <strong>Zhipeng Cai</strong>, Frank Neumann <br />
International Journal on Computer Vision (IJCV), <Highlight>Special Issue on Best of ECCV 2018</Highlight>. <br />
<a href='https://rd.springer.com/content/pdf/10.1007%2Fs11263-019-01207-y.pdf'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>

<!-- <div class="paper" id="ICCV19"><img class="paper" src="papers/ICCV19.png" title="ICCV19" />
<div> <strong>[ICCV19 <Highlight>oral (top 4.6%)</Highlight>]</strong> Consensus Maximization Tree Search Revisited <br />
<strong>Zhipeng Cai</strong>, Tat-Jun Chin, Vladlen Koltun <br />
International Conference on Computer Vision (ICCV) 2019. <br />
<a href='https://arxiv.org/abs/1908.02021'>[Arxiv preprint]</a>
<a href='https://github.com/ZhipengCai/MaxConTreeSearch'> [Code] </a>
<br />
<iframe width="400" height="225" src="https://youtube.videoken.com/embed/my3jocjpD0U?tocItem=157&autoplay=0" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div> -->
<div class="paper" id="ICCV19">
    <img class="paper" src="papers/ICCV19.png" title="ICCV19" />
    <div>
        <strong>[ICCV19 <Highlight>oral (top 4.6%)</Highlight>]</strong> Consensus Maximization Tree Search Revisited <br />
        <strong>Zhipeng Cai</strong>, Tat-Jun Chin, Vladlen Koltun <br />
        International Conference on Computer Vision (ICCV) 2019 <br />
        <a href='https://arxiv.org/abs/1908.02021'>[Arxiv preprint]</a>
        <a href='https://github.com/ZhipengCai/MaxConTreeSearch'> [Code] </a>
        <br />

        <!-- This div will be replaced with the YouTube video iframe -->
        <div id="player"></div>

        <script src="https://www.youtube.com/iframe_api"></script>
        <script>
            var player;
            var initialized = false;

            function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                    height: '225',
                    width: '400',
                    videoId: 'my3jocjpD0U',
                    playerVars: {
                        'start': 5633,
                        'autoplay': 0
                    },
                    events: {
                        'onReady': onPlayerReady
                    }
                });
            }

            function onPlayerReady(event) {
                event.target.playVideo();
		player.pauseVideo();
            }

            // function onPlayerStateChange(event) {
            //     if (event.data == YT.PlayerState.PLAYING && !initialized) {
            //         player.pauseVideo();
            //         initialized = true;
            //     }
            // }
        </script>
    </div>
    <div class="spanner"></div>
</div>






<div class="paper" id="ISPRSJ18"><img class="paper" src="papers/Reg_Arch_visual_4.png" title="ISPRSJ18" />
<div> <strong>[ISPRSJ]</strong> Practical Optimal Registration of Terrestrial LiDAR Scan Pairs<br />
<strong>Zhipeng Cai</strong>, Tat-Jun Chin, Alvaro Parra Bustos, Konrad Schindler <br />
ISPRS Journal of Photogrammetry and Remote Sensing, 2019. <br />
<a href='http://arxiv.org/abs/1811.09962'>[Arxiv preprint]</a>
<a href='https://www.sciencedirect.com/science/article/pii/S0924271618303125?via%3Dihub'>[PDF]</a> 
<a href='https://github.com/ZhipengCai/Demo---Practical-optimal-registration-of-terrestrial-LiDAR-scan-pairs'>[Code]</a> 
<!--<a href='https://youtu.be/MKzSN4bbs1o'>[Video]</a>--> 
<br />
<iframe width="400" height="225" src="https://www.youtube.com/embed/MKzSN4bbs1o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>

<h2 id="confpapers">2018</h2>

<div class="paper" id="ECCV18_IBCO"><img class="paper" src="papers/ECCV18_IBCO.png" title="ECCV18_IBCO" />
<div> <strong>[ECCV18 <Highlight>oral (top 2.4%)</Highlight>]</strong> Deterministic consensus maximization with biconvex programming<br />
<strong>Zhipeng Cai</strong>, Tat-Jun Chin, Huu Le, David Suter <br />
European Conference on Computer Vision (ECCV) 2018 <br />
<a href='https://arxiv.org/abs/1807.09436'>[Arxiv preprint]</a> 
<a href='https://github.com/ZhipengCai/Demo---Deterministic-consensus-maximization-with-biconvex-programming'> [Code] </a>
<a href='https://drive.google.com/open?id=1Cm0mHk52RkGB23rslaYiFoy7Md4XaXzx'> [Slides] </a>
<br />
<iframe width="400" height="225" src="https://www.youtube.com/embed/gE6zkCEC9dA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="ECCV18_Hardness"><img class="paper" src="papers/ECCV18_Hardness.png" title="ECCV18_Hardness" />
<div> <strong>[ICCV19 <Highlight>oral (top 2.4%)</Highlight>]</strong> Robust fitting in computer vision: easy or hard?<br />
Tat-Jun Chin, <strong>Zhipeng Cai</strong>, Frank Neumann <br />
European Conference on Computer Vision (ECCV) 2018 <br />
<Highlight>Selected as one of the 12 best papers from the conference (0.4% acceptance rate) </Highlight><br />
<a href='https://arxiv.org/abs/1802.06464'>[Arxiv preprint]</a> 
<a href='https://drive.google.com/open?id=14muETTtSj-yMGR_PLRmCgC4sqXN1Hi8p'> [Slides] </a>
<br />
<iframe width="400" height="225" src="https://www.youtube.com/embed/VpJ56rsimzM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="spanner"></div>
</div>

<h2 id="confpapers">2016</h2>

<div class="paper" id="ITS16_Wen">
<div> Spatial-Related Traffic Sign Inspection for Inventory Purposes Using Mobile Laser Scanning Data<br />
Chenglu Wen, Jonathan Li, Huan Luo, Yongtao Yu, <strong>Zhipeng Cai</strong>, Hanyun Wang, Cheng Wang <br />
IEEE Transactions on Intelligent Transportation Systems, 2016 <br />
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7089251'>[PDF]</a> <br />
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="ITS16_Luo">
<div> Patch-based semantic labeling of road scene using colorized mobile LiDAR point clouds<br />
Huan Luo, Cheng Wang, Chenglu Wen, <strong>Zhipeng Cai</strong>, Ziyi Chen, Hanyun Wang, Yongtao Yu, Jonathan Li <br />
IEEE Transactions on Intelligent Transportation Systems, 2016 <br />
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7345561'>[PDF]</a> <br />
</div>
<div class="spanner"></div>
</div>	
	
	
<h2 id="confpapers">2015</h2>
	
<div class="paper" id="GRSL16"><img class="paper" src="papers/GRSL16.png" title="Occluded Boundary Detection for Small-footprint Ground-borne LIDAR Point Cloud Guided by Last-echo" />
<div>Occluded Boundary Detection for Small-footprint Ground-borne LIDAR Point Cloud Guided by Last-echo<br />
<strong>Zhipeng Cai</strong>, Cheng Wang, Chenglu Wen, Jonathan Li   <br />
IEEE Geoscience and Remote Sensing Letters, 2015 <br />
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7226818'>[PDF]</a> <br />
</div>
<div class="spanner"></div>
</div>	
	
<div class="paper" id="ICSDM15"><img class="paper" src="papers/ICSDM15.png" title="3D-PatchMach: an Optimization Algorithm for Point Cloud Completion" />
<div>
3D-PatchMach: an Optimization Algorithm for Point Cloud Completion<br />
<strong>Zhipeng Cai</strong>, Cheng Wang, Chenglu Wen, Jonathan Li   <br />
Second IEEE International Conference on Spatial Data Mining and Geographical Knowledge Services(ICSDM2015)<br />
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298044'>[PDF]</a><br /> 
</div>
<div class="spanner"></div>
</div>


<h2 id="confpapers">2012</h2>
	
<div class="paper" id="CVRS12_Wang">
<div> Automatic road extraction from mobile laser scanning data<br />
Hanyun Wang, <strong>Zhipeng Cai</strong>, Huan Luo, Cheng Wang, Peng Li, Wentao Yang, Suoping Ren, Jonathan Li   <br />
International Conference on Computer Vision in Remote Sensing (CVRS), 2012<br />
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6421248'>[PDF]</a><br /> 
</div>
<div class="spanner"></div>
</div>
	
	
<div class="paper" id="CVRS12_Li_Scale">
<div> Scale invariant kernel-based object tracking<br />
 Peng Li,  <strong>Zhipeng Cai</strong>, Hanyun Wang, Zhuo Sun, Yunhui Yi, Cheng Wang, Jonathan Li <br />
International Conference on Computer Vision in Remote Sensing (CVRS), 2012<br />
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6421270'>[PDF]</a><br /> 
</div>
<div class="spanner"></div>
</div>
	
	
<div class="paper" id="CVRS12_Li_Cascade">
<div> Cascade framework for object extraction in image sequences<br />
 Peng Li, <strong>Zhipeng Cai</strong>, Cheng Wang, Zhuo Sun, Hanyun Wang, Jonathan Li <br />
International Conference on Computer Vision in Remote Sensing (CVRS), 2012<br />
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6421233'>[PDF]</a><br /> 
</div>
<div class="spanner"></div>
</div>
	
	
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Patent:</h2>
<div class="paper">
<ul>
<p>1. Interactive three-dimensional point cloud color editing method. Publication number: <a href='https://www.google.com.au/patents/CN103489224A?cl=en&dq=Interactive+three-dimensional+point+cloud+color+editing+method&hl=zh-CN&sa=X&ved=0ahUKEwiFuIeNsbvQAhVBPY8KHRlyCWsQ6AEIGjAA'>CN 103489224 A</a><br /></p>
<p>2. Three-dimensional point cloud auto-completion method. Publication number: <a href='http://patentool.wanfangdata.com.cn/Patent/Details?id=CN201410308951.5'>CN 104063898 A</a><br /></p>
<p>3. <a href='https://www.freepatentsonline.com/y2023/0326197.html'>TECHNOLOGY TO CONDUCT CONTINUAL LEARNING OF NEURAL RADIANCE FIELDS</a><br /></p>	
</ul>
	
</div>
</div>
</div>

<div style="clear: both;">
  <div class="section">
    <h2 id="confpapers">Supervised students/interns</h2>
    <div style="text-align: center;">
      <div style="display: inline-block; width: 200px; margin-right: 10px" class="wp-caption alignleft">
        <img class="size-full wp-image-436" title="Ashish Seth" src="people/ashish_seth.jpg" alt="Ashish Seth" width="150" height="150" />
        <p class="wp-caption-text"><a href="https://scholar.google.com/citations?user=aBn1e34AAAAJ&hl=en">Ashish Seth</a><br />University of Maryland</p>
      </div>
      <div style="display: inline-block; width: 200px; margin-right: 10px" class="wp-caption alignleft">
        <img class="size-full wp-image-436" title="Junda Cheng" src="people/junda_cheng.jpg" alt="Junda Cheng" width="150" height="150" />
        <p class="wp-caption-text"><a href="https://scholar.google.com/citations?user=_G_Tu9EAAAAJ&hl=zh-CN">Junda Cheng</a><br />HUST</p>
      </div>
      <div style="display: inline-block; width: 200px; margin-right: 10px" class="wp-caption alignleft">
        <img class="size-full wp-image-436" title="Tam Nguyen" src="people/tam_nguyen.jpg" alt="Tam Nguyen" width="150" height="150" />
        <p class="wp-caption-text">Tam Nguyen<br />University of Adelaide</p>
      </div>
      <div style="display: inline-block; width: 200px; margin-right: 10px" class="wp-caption alignleft">
        <img class="size-full wp-image-436" title="Xuelun Shen" src="people/xuelun_shen.jpg" alt="Xuelun Shen" width="150" height="150" />
        <p class="wp-caption-text"><a href="https://xuelunshen.com/">Xuelun Shen</a><br />Xiamen University</p>
      </div>
      <div style="display: inline-block; width: 200px; margin-right: 10px" class="wp-caption alignleft">
        <img class="size-full wp-image-436" title="Yixing Lao" src="people/yixing_lao.jpg" alt="Yixing Lao" width="150" height="150" />
        <p class="wp-caption-text"><a href="https://yxlao.github.io/">Yixing Lao</a><br />University of Hong Kong</p>
      </div>
      <div style="display: inline-block; width: 200px; margin-right: 10px" class="wp-caption alignleft">
        <img class="size-full wp-image-436" title="Motasem Alfarra" src="people/motasem_alfarra.jpg" alt="Motasem Alfarra" width="150" height="150" />
        <p class="wp-caption-text"><a href="https://motasemalfarra.netlify.app/">Motasem Alfarra</a><br />KAUST</p>
      </div>
      <div style="display: inline-block; width: 200px; margin-right: 10px" class="wp-caption alignleft">
        <img class="size-full wp-image-436" title="Ameya Prabhu" src="people/ameya_prabhu.jpg" alt="Ameya Prabhu" width="150" height="150" />
        <p class="wp-caption-text"><a href="https://drimpossible.github.io/">Ameya Prabhu</a><br />University of Oxford</p>
      </div>
    </div>
  </div>
</div>	

	
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Working experience</h2>
<div class="paper">
<ul>
<li>Jan-2019 to June-2020: Internship at <a href = 'http://vladlen.info/lab/'>Intel Intelligent Systems Lab</a> at Santa Clara, CA, USA. Working with Dr. <a href = 'http://vladlen.info/'>Vladlen Koltun</a>.</li>
<li>July-2020 to May-2022: Postdoc at <a href = 'http://vladlen.info/lab/'>Intel Intelligent Systems Lab</a> at Santa Clara, CA, USA. Working with Dr. <a href = 'http://vladlen.info/'>Vladlen Koltun</a>.</li>
<li>June-2022 to Nov-2024: Staff Research scientist at Intel Embodied AI Lab at Santa Clara, CA, USA. </li>
<li>Nov-2024 to Now: Senior AI Research scientist at Meta, Menlo Park, CA, USA. </li>

</ul>
<div class="spanner"></div>
</div>
</div>
</div>
	
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Academic Service</h2>
<div class="paper">
<ul>
<li>Conference reviewer: <a href='https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition'>CVPR</a>, <a href='https://en.wikipedia.org/wiki/International_Conference_on_Computer_Vision'>ICCV</a>, <a href='https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision'>ECCV</a>, <a href='http://www.aaai.org/Conferences/conferences.php'>AAAI</a>, <a href='https://iclr.cc/'>ICLR</a>, <a href='https://https://icml.cc//'>ICML</a>, <a href='https://neurips.cc/'>NeurIPs</a> </li>
<li>Journal reviewer: <a href='https://mc.manuscriptcentral.com/tpami-cs'>TPAMI</a>, <a href= 'https://link.springer.com/journal/11263'> IJCV</a>, <a href='https://www.journals.elsevier.com/isprs-journal-of-photogrammetry-and-remote-sensing'>ISPRS Journal of Photogrammetry and Remote Sensing</a>, <a href='https://www.journals.elsevier.com/pattern-recognition'>Pattern Recognition</a>, <a href='https://www.ieee-ras.org/publications/ra-l'>RA-L</a> </li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 22th Nov, 2016</font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=5eyy&d=BGOr6AsJEFlhvzuajVPFK8a99dHE3Edu1vy9kmu6b3M"></script>

</body>
</html>

    Contact GitHub API Training Shop Blog About 

    © 2016 GitHub, Inc. Terms Privacy Security Status Help 

